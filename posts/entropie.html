<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>
    <title>Entropie</title>
</head>

<body>

    <div class="container cmath">

        <div class="alert alert-secondary">
            <h2 class="alert-heading">Qu'est-ce que l'entropie ?</h2>
        </div>

        <p> Cet article a pour ambition de
            permettre au lecteur profane d'appréhender
            l'idée d'entropie en le guidant dans sa réflexion personnelle. Ce n'est ni un exposé encyclopédique, ni un
            cours, bien qu'il soit inspiré de l'introduction à un cours de thermodynamique donné à un niveau
            universitaire. Pour l'essentiel du propos, peu de calculs sont en jeu.</p>

        <p>
            L'entropie est parfois présentée comme une mesure du désordre. Notre but est ici d'aider lecteur à se
            convaincre qu'une telle interprétation est inadaptée à la compréhension de la nature de l'entropie.
        </p>

        <p>Une approche statistique est employée et nous nous appuierons sur des simulations programmées en langage
            Python dont les scripts sont disponibles <a href="scripts_entropie.ipynb">ici</a>.</p>

        <p>Bonne lecture.</p>

        <h3>
            Pile ou Face ?
        </h3>

        <p>
            Pour commencer, utilisons un jeu très simple, celui de <i>Pile ou Face</i>.
        </p>

        <p>
            Si nous lançons une pièce un grand nombre de fois et que cette pièce est bien équilibrée, nous nous
            attendons à observer
            <i>Pile</i> et <i>Face</i> dans des proportions très proches.
        </p>

        <p>
            Sur le graphique ci-dessous, obtenu à l'aide d'une simulation de 10000 lancers, sont réprésentés le
            nombre de tirages où les côtés <i>Pile</i> et <i>Face</i> ont été obtenus.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_1.png" style="width: 300px;">
        </center>

        <p>
            Comme nous le prévoyions, les nombres de fois où <i>Pile</i> et <i>Face</i> ont été observés sont très
            proches. Pour cette simulation, 5010 fois <i>Face</i> et 4090 fois <i>Pile</i>.
        </p>

        <p>
            Renouvelons le jeu, mais cette fois-ci lançons deux pièces par tirage.
            Quatre résultats sont possibles : <i>Pile-Pile</i> (PP), <i>Pile-Face</i> (PF),
            <i>Face-Pile</i> (FP) et <i>Face-Face</i> (FF).
            Si nous procédons à de multpiles tirages avec deux pièces, nous nous attendons à observer PP, PF, FP et FF
            un nombre similaire de fois. Ces quatre arrangements étant en effet équiprobables, avec la probabilité
            `1/4`, c'est-à-dire une
            chance sur quatre.
        </p>

        <p>
            Le graphique ci-dessous est là encore le résultat d'une simulation qui a réalisé 10000 tirages avec deux
            pièces. Il montre le nombre de fois où chacun des quatre arrangements a été observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2_tous_tirages.png" style="width: 300px;">
        </center>

        <p>
            Nous remarquons que chaque arrangement FF, FP, PF et PP a été obtenu environ 2500 fois, soit près d'un quart
            du temps,
            comme le laissait prévoir la probabilité `1/4`.
        </p>

        <p>
            Jusqu'ici nous nous sommes intéressés aux arrangements ordonnés des pièces. C'est-à-dire que l'ordre des
            pièces importait. En effet, les tirages PF et FP étaient considérés comme distincts. Pour comprendre
            l'entropie, nous devons aussi nous intéresser aux arrangements non ordonnés. Clarifions donc ces termes
            <b>ordonné</b> et <b>non ordonné</b>.
        </p>

        <p>
            L'illustration ci-dessous représente deux tirages à cinq pièces disposées dans deux arrangements
            ordonnés. En haut, l'arrangement <i>Pile Face Pile Pile Pile</i> (PFPPP). En bas, l'arrangement <i>Pile Pile
                Face Pile
                Pile</i> (PPFPP). Nous faisons la disctinction entre les deux arrangements car les pièces sont disposées
            dans un
            certain ordre.
        </p>

        <center>
            <img src="images_entropie/Tirages_5_pieces_ordonnees.png" style="width: 300px;">
        </center>

        <br />

        <p>
            Dans l'illustration ci-dessous, la dispositions des pièces change d'un arrangement à l'autre. Cela ne nous
            permet pas de définir un ordre sans ambiguïté. Cependant, nous remarquons que les deux arrangements sont
            deux combinaisons avec quatre <i>Pile</i> et une <i>Face</i>. Nous parlerons alors de l'arrangement non
            ordonné 4P1F. Nous dirons par ailleurs que PFPPP et PPFPP sont deux combinaisons possibles de 4P1F.
        </p>

        <center>
            <img src="images_entropie/Tirages_5_pieces_non_ordonnees.png" style="width: 300px;">
        </center>

        <br />

        <p>
            Revenons aux tirages avec deux pièces.
            Les arrangements ordonnés PF et FP correspondent à un unique arrangement non ordonné avec une
            pièce sur <i>Pile</i> et une pièce sur <i>Face</i> (1P1F). Les trois arrangements non ordonnés sont donc :
            deux
            fois <i>Pile</i> (2P), deux fois <i>Face</i> (2F) et une fois <i>Pile</i> et une fois <i>Face</i> (1P1F).
            Leurs probabilités respectives sont `1/4`, `1/4` et `1/2`.
        </p>

        <p>
            La simulation avec 10000 tirages de deux pièces a été réanalysée mais cette fois en s'intéressant aux
            arrangements non ordonnés.
            Le graphique ci-dessous résume cette analyse en montrant le nombre de fois où chacun des trois arrangements
            2F, 2P et 1P1F a été observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2.png" style="width: 300px;">
        </center>

        <p>
            Comme l'indiquaient les probabilités mentionnées ci-dessus,
            c'est bien l'arrangement avec une pièce <i>Pile</i> et une pièce <i>Face</i> qui est le plus observé
            et ce près de la moitié du temps (4979 fois pour la simulation présentée).
            Les combinaisons avec les deux pièces identiques 2P et 2F sont quant à elles observées un nombre similaire
            de fois, environ égal à 2500 chacune.
        </p>

        <p>
            Étudions maintenant le nombre de fois où les arrangements non ordonnés possibles ont été obtenus en
            utilisant 4, puis 16 pièces par tirage, toujours en ayant simulés 10000 tirages.
        </p>

        <p>
            Dans le cas du jeu à 4 pièces, on peut remarquer ci-dessous que c'est l'arrangement non ordonné avec deux
            fois
            <i>Face</i> et deux fois <i>Pile</i> (2F2P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_4.png" style="width: 300px;">
        </center>

        <p>
            Dans le cas du jeu à 16 pièces ci-dessous, c'est l'arrangement avec huit fois <i>Face</i> et huit fois
            <i>Pile</i>
            (8F8P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_16.png" style="width: 350px;height: 175px;">
        </center>

        <p>
            Si nous réalisions des simulations avec un nombre encore plus grand de pièces, nous trouverions encore que
            l'arrangement non ordonné le plus observé serait celui pour lequel le nombre de fois <i>Pile</i> est égal au
            nombre de fois <i>Face</i> (pour un nombre de pièces pair). Par exemple, avec 100 pièces, nous trouverions
            que
            l'arrangement le plus probable serait 50 fois <i>Face</i> et 50 fois <i>Pile</i> (50F50P). Et si le nombre
            de pièces
            était impair ? Quels arrangements seraient les plus probables ?
        </p>

        <p>
            Résumons ce que les tirages de pièces nous apprennent.
        <ul>
            <li>
                Pour chaque pièce, la probabilité de tirer <i>Pile</i> est toujours d'une chance
                sur deux. Il en va de même avec la probabilité de tirer <i>Face</i>.
            </li>
            <li>
                Quel que soit le nombre de pièces, les arrangements ordonnés sont équiprobables avec la probabilité
                `1/2^n` où <i>n</i> est le nombre de pièces. Par exemple, dans un jeu à cinq pièces nous n'avons ni
                plus ni moins de chance de tirer PFPPP, PPFPP que de tirer FFFFF. Concrètement, une chance sur `2^5`,
                soit une chance sur 32.
            </li>
            <li>
                Si nous nous intéressons aux arrangements non ordonnés, le comportement de l'ensemble de pièces prises
                au hasard devient bien plus prédictible.
                En effet, l'arrangement non ordonné le plus probable aura autant de <i>Pile</i> que de <i>Face</i>.
                Par exemple, dans un jeu à 16 pièces, nos chances sont maximum en misant sur 8P8F alors qu'il est bien
                plus hasardeux de miser sur 16F ou même 14F2P.
            </li>

        </ul>

        </p>

        <p>
            Nous pouvons donc conclure que dans ce jeu de hasard, l'arrangement non ordonné avec autant de côtés
            <i>Pile</i>
            que de côtés <i>Face</i> a un
            statut bien particulier. C'est cet arrangement qui est le plus probable. Souvenez-vous par exemple qu'avec
            deux pièces l'arrangement 1F1P est le plus probable puisqu'il correspond à FP et PF, alors que 2P et 2F
            qui correspondent à PP et FF sont uniques. Nous verrons par le calcul que l'arrangement non ordonné le plus
            probable est en effet celui qui possède le plus grand nombre d'arrangements ordonnés.
        </p>

        <h2>
            Et l'entropie dans tout ça ?
        </h2>

        <p>
            Pour faire connaissance avec l'entropie, nous devons pénétrer sur son territoire, celui de la
            thermodynamique statistique pour y emprunter quelques éléments de langage. Notez bien que la thermodynamique
            statistique traite
            de systèmes physiques qui contiennent un nombre de particules de l'ordre du nombre d'Avogradro, donc bien
            loin des quelques pièces dont nous avons discuté jusqu'ici. Cependant, nous nous permettons d'ouvrir la
            frontière entre les deux mondes à des fins de compréhension.
        </p>

        <p>
            Jetons <i>n</i> pièces au hasard. Le nombre de pièces qui vont attérir sur une face donnée n'est
            pas contraint. Nous disons en thermodynamique statistique que c'est un degré de liberté. Dans le cas de nos jets
            de pièces, celui-ci peut varier entre 0 et <i>n</i>.
        </p>
        <p>
            Pour une valeur donnée du degré de liberté, il existe un unique arrangement non ordonné. Par exemple, nous
            jetons 16 pièces et définissons le nombre de pièces attérissant côté <i>Pile</i> comme degré de liberté. Celui-ci peut en
            principe prendre n'importe quelle valeur entre 0 et 16. Admettons que 5 pièces tombent sur <i>Pile</i>. La
            valeur 5 correspond à l'arrangement non ordonné 5P11F. En thermodynamique statistique, nous parlerions de
            macro-état pour 5P11F. Un macro-état correspond à une valeur du degré de liberté.
        </p>
        <p>
            Nous avons appris qu'un arrangement non ordonné donné correspond à des arrangements ordonnés, tous
            équiprobables. Pour 5P11F, un des 65536 arrangements ordonnés possibles est PPFFPPFFPFFFFFFF.
            En thermodynamique statistique, nous parlerions de micro-états pour les arrangements ordonnés.
        </p>
        <p>
            Pour résumer, une valeur du degré de liberté correspond
            à un macro-état, lequel correspond à un ou plusieurs micro-états.
        </p>

        <p>
            Nous avons découvert que l'ensemble de pièces jetées au hasard ne va pas attérir pour donner n'importe
            quelle valeur du degré de liberté. La valeur la plus probable que va prendre celui-ci est celle qui
            correspond au macro-état avec le plus grand nombre de micro-états. En effet, c'est ce macro-état qui est le
            plus probable. Si nous jetons par exemple 16 pièces, nous avons vu qu'il est fort probable que le nombre de
            pièce tombées côté <i>Pile</i> soit 8 car le macro-état 8F8P est le plus probable.
        </p>

        <p>
            Nous venons sans le savoir de donner une formulation du deuxième principe de la thermodynamique. Les
            systèmes tendent à
            s'équilibrer dans le macro-état le plus probable, celui qui a le plus grand nombre de micro-états.
        </p>
        <p>
            A l'instar de Boltzmann, désignons par <i>W</i> le nombre de micro-états d'un macro-état donné et par
            <i>S</i> l'entropie. La formule de Boltzmann `S = k_B ln W` permet de calculer l'entropie comme le
            produit du logarithme népérien de <i>W</i> (`ln W`) et de la constante de Boltzmann (`k_B`).
        </p>
        <p>
            Mettons de côté le logarithme et la constante `k_B` pour formuler les conclusions importantes auxquelles
            nous
            sommes parvenus : <b>Les systèmes tendent à s'équilibrer dans le macro-état qui rend l'entropie maximum.
                L'entropie d'un macro-état mesure la variété de réalisations de ce celui-ci en termes de
                micro-états.</b>
            Si une phrase devait être retenue à propos de l'entropie, ce serait cette dernière.
        </p>

        <h3>
            Prêts pour un test de compréhension ?
        </h3>

        <p>
            Pour renforcer notre compréhension de ce qui précède, intéressons-nous à un système de 20 jetons placés au
            hasard sur une grille de 10`times`10 cases, une case ne pouvant accueillir plus d'un jeton à la fois. Deux
            arrangements sont proposés ci-dessous.
        </p>

        <center>
            <img src="images_entropie/Pions_grille10x10_A.png" style="width: 250px">
        </center>
        </br>
        <center>
            <img src="images_entropie/Pions_grille10x10_B.png" style="width: 250px;">
        </center>

        </br>

        <p>
            Question (piège) : Laquelle des dispositions A ou B a la plus grande entropie ?
        </p>

        <p>
            Réponse : Ni l'une, ni l'autre. Les deux sont deux micro-états d'un même macro-état. Pour obtenir l'entropie
            de ce macro-état il faut calculer le nombre de combinaisons correspondant au placement de 20 jetons dans
            100 cases.
        </p>

        <p>
            Question (difficile) : Définissons le degré de liberté comme le nombre de jetons à placer sur la grille.
            Partant du macro-état ci-dessus avec 20 jetons sur 100 cases, comment générer un macro-état avec une
            entropie plus grande ?
        </p>

        <p>
            Indice : Remplir des cases avec des jetons est équivalent à arranger des jetons et des cases vides. Arranger
            des cases vides et des jetons, c'est très semblable à arranger des pièces côté <i>Pile</i> et des pièces
            côté <i>Face</i>.
        </p>

        <p>
            Réponse : Comme l'indice le suggère calculer l'entropie du macro-état correspondant à 20 pièces sur 100
            cases, c'est comme calculer l'entropie pour 20 pièces sur <i>Pile</i> et 80 pièces sur <i>Face</i>. Comme
            nous l'avons vu, le nombre de micro-états, et donc l'entropie, est maximum pour 50 pièces sur <i>Pile</i> et
            50 pièces sur <i>Face</i>. Partant de 20 jetons pour le degré de liberté, augmenter celui-ci génère un
            macro-état d'entropie plus grande. Nous ne devons cependant pas dépasser 79 jetons. Souvenez-vous des
            pièces. Le nombre d'arrangements ordonnés pour 80P20F est le même que pour 20P80F. Ainsi l'entropie pour 20
            jetons est la même que pour 80. En conclusion, partant de 20 jetons, l'entropie augmentera en utilisant
            entre 21 et 79 jetons. Le maximum d'entropie étant obtenu avec 50 jetons.
        </p>

        <h3>
            Entropie et désordre
        </h3>

        <p>
            Il est fréquent de lire que l'entropie serait une mesure du désordre dans un système donné. Un
            exemple très qualitatif qui revient souvent est celui qu'une chambre d'enfant en désordre a une entropie
            plus grande que lorsqu'elle est rangée. La discussion ci-dessus devrait nous avoir armés pour comprendre ce qui
            est incorrect dans de telles affirmations.
        </p>

        <p>
            Reprenons les dispositions de jetons du paragraphe précédent. Nous pourrions dire que la disposition A est
            en désordre au contraire de la disposition B dont le motif est très précis. Toutefois nous savons maintenant
            que ni la disposition A, ni la disposition B n'ont d'entropie. Ce sont deux arrangements possibles de 20
            jetons dans une grille de 100 cases. Qui plus est, elles sont équiprobables si elles sont générées au
            hasard.
        </p>
        <p>
            Revenons à la chambre d'enfant. Voyez-vous pourquoi elle n'a pas moins d'entropie quand elle est rangée
            que lorsqu'elle est en désordre ? La configuration "chambre rangée" est un micro-état du même macro-état que
            toutes
            celles jugées en désordre. Nous pouvons calculer l'entropie de la chambre avec un certain nombre
            d'objets et un certain nombre d'emplacements où les ranger, mais pour un nombre d'objets et d'emplacements
            donnés, qu'elle soit rangée ou en désordre ne change pas son entropie.
        </p>

        <p>
            Nous le voyons, penser à l'entropie en termes de mesure du désordre n'est pas adapté. En adoptant ce point
            de vue, nous nous forçons à réfléchir en termes de combinaison unique, alors que l'entropie mesure le nombre
            de combinaisons différentes qui correspondent à un arrangement donné.
        </p>

        <h3>Si vous aimez le calcul...</h3>

        <p>
            Si nous lançons quatre pièces au hasard, comment sait-on que 2F2P correspond au plus grand nombre
            d'arrangements
            ordonnés que l'on peut obtenir avec ces quatre pièces ? De manière plus générale, si nous jouons avec
            <i>n</i> pièces, si <i>k</i> pièces tombent côté <i>Pile</i>, combien d'arrangements ordonnés correspondent à
            <i>k</i>P(<i>n-k</i>)F ?
        </p>

        <p>
            En langage mathématique, nous recherchons le nombre de combinaisons de <i>k</i> éléments dans un ensemble de
            <i>n</i> éléments avec `k>=n`. Ce nombre est `C_n^k = {n!} / {k!(n-k!)}`.
        </p>

        <p>
            Pour le jeu à quatre pièces, le tableau ci-dessous donne les valeurs du nombre de combinaisons pour chaque
            arrangement non ordonné possible. Le lecteur averti remarquera que ces nombres sont ceux du triangle de
            Pascal pour <i>n</i>=4.
        </p>

        <table class="table table-striped">
            <thead>
                <tr>
                    <th>Arrangement </th>
                    <th>Nombre de combinaisons&nbsp;</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>4P</td>
                    <td>`C_4^0 = 1` </td>
                </tr>
                <tr>
                    <td>3P1F</td>
                    <td>`C_4^1 = 4`</td>
                </tr>
                <tr>
                    <td>2P2F</td>
                    <td>`C_4^2 = 6`</td>
                </tr>
                <tr>
                    <td>1P3F</td>
                    <td>`C_4^3 = 4`&nbsp;</td>
                </tr>
                <tr>
                    <td>4F</td>
                    <td>`C_4^4 = 1`</td>
                </tr>
            </tbody>
        </table>

        <p>
            Nous constatons bien que pour quatre pièces, le nombre de combinaisons maximum est obtenue avec deux pièces
            côté <i>Pile</i> et deux pièces côté <i>Face</i> (2P2F).
        </p>

        <p>
            Comment démontrer dans le cas général où <i>n</i> pièces (<i>n</i> pair) sont jetées que c'est l'arrangement
            avec `n/2` pièces sur <i>Pile</i> qui a le plus grand nombre d'arrangements ordonnés ? Il s'agit de
            rechercher pour quelle valeur de <i>k</i> `C_n^k` est maximum et de montrer que ce maximum est unique. Cela
            requiert plusieurs lignes de calcul et un peu de temps.
        </p>

        <p>Question : Une simulation place des jetons au hasard sur une grille de 16 cases. De combien augmente
            l'entropie en passant de trois à huit jetons ?</p>

        <p>Réponse : Avec trois jetons, l'entropie est proportionnelle à `ln C_16^3 = ln 560`. Avec huit jetons elle est
            proportionnelle à `ln C_16^8 = ln 12870`. En passant de trois jetons à huit jetons, l'entropie a été
            multipliée par environ 1,5.</p>

        <p>Question : Une simulation place au hasard 4 jetons sur une grille de 16 cases. Quelle est la probabilité
            d'obtenir un jeton à chaque coin ? La probabilité d'obtenir les quatre jetons au centre ?</p>

        <p>Réponse : Les deux dipositions "un jeton dans chaque coin" et "tous les jetons au centre" sont équiprobables.
            Le nombre de combinaisons qui correspondent à quatre jetons sur 16 cases est `C_16^4 = (16!)/(4!12!) =
            1820`. La probabilité d'obtenir l'une des dispositions est donc `1/C_16^4`, soit une chance sur 1820.</p>

        <p>Question : Jouons à <i>Pile ou Face</i> avec 16 pièces. Quelle est la probabilité d'obtenir 15P1F? Quelle est
            la probabilité d'obtenir 8P8F ?</p>

        <p>Réponse : Le nombre total de combinaisons possibles avec 16 pièces est `2^16 = 65536`. La probabilité
            d'obtenir 15P1F est `C_16^1 / 2^16 = 1/65536 approx 0,0015%`. La probabilité d'obtenir 8P8F est `C_16^8 /
            2^16 = 12870/65536 approx 20%`, soit plus de 10000 fois plus que la probabilité d'obtenir 1P15F.</p>

    </div>

    <div class="jumbotron text-center" style="margin-bottom:0">
        <p><a href="https://joulik.github.io/">joulik.github.io</a></p>
    </div>

</body>

</html>