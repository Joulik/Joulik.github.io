<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="mystyle.css">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>

    <script src="progressbar.js"></script>

    <title>Entropie</title>
</head>

<body>

    <div id="progress-bar"></div>

    <div class="container cmath">

        <div class="alert alert-secondary">
            <h2 class="alert-heading">L'entropie, une mesure du désordre ?</h2>
        </div>

        <p>
            L'entropie est parfois présentée comme une mesure du désordre. Le but est ici de se
            convaincre qu'une telle interprétation est en général inadaptée à la compréhension de la nature de l'entropie.
        </p>

        <p> Cet article a pour ambition de permettre au lecteur profane d'appréhender l'idée d'entropie en le guidant 
            dans sa réflexion personnelle. Une approche mathématique est employée. Cependant, pour l'essentiel du propos, 
            peu de calculs sont en jeu. 
        </p>
        
        <p>
            D'autres articles discuteront de l'entropie en lien avec des systèmes physiques
        </p>
             
        <p>    
            Les résultats de simulations sont utilisés à des fins d'illustration.
            Les scripts en langage Python de ces simulations sont disponibles 
            <a href="https://github.com/Joulik/Joulik.github.io/blob/master/posts/scripts_entropie.ipynb">ici</a>.
        </p>

        <p>Bonne lecture.</p>

        <h3>
            Pile ou Face ?
        </h3>

        <p>
            Utilisons pour commencer un jeu très simple, <i>Pile ou Face</i>.
        </p>

        <p>
            Lorsqu'on lance une pièce bien équilibrée, on a une chance sur deux de tirer <i>Pile</i> 
            et une chance sur deux de tirer <i>Face</i>. En d'autres termes, la probabilité d'observer <i>Pile</i> 
            et la probabilité d'observer <i>Face</i> sont égales et valent `1/2`.
        </p>

        <p>
            Que va-t-on observer si on lance une pièce un grand nombre de fois, par exemple 10000 fois, ou si on demande à 
            10000 personnes de jeter chacune une pièce ?
        </p>

        <p>
            C'est une tâche assez longue que l'on peut facilement confier à un ordinateur par le biais d'une simulation.
            On réalise ainsi en une fraction de seconde ce qui prendrait des heures dans la réalité.
        </p>
        
        <p>
            Le graphique ci-dessous a été obtenu à l'aide d'un programme informatique qui a simulé 10000 lancers.
            Y sont réprésentés le nombre de fois où <i>Pile</i> et <i>Face</i> ont été observés.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_1.png" style="width: 300px;">
        </center>

        <p>
            Pour cette simulation, <i>Face</i> a été observé 5010 fois et <i>Pile</i> a été observé 4990 fois.
            Peut-être aviez-vous anticipé ce résultat ? Le fait que <i>Pile</i> et <i>Face</i> sont observés 
            dans des proportions très proches provient du fait que chaque lancer de pièce individuel a une probabilité 
            égale d'aboutir à <i>Pile</i> ou <i>Face</i> .
        </p>

        <p>
            Renouvelons le jeu, mais cette fois-ci en lançant successivement deux pièces et en notant l'ordre des résultats.
            Quatre résultats sont possibles : <i>Pile</i> puis <i>Pile</i> (noté PP), <i>Pile</i> puis <i>Face</i> (noté PF),
            <i>Face</i> puis <i>Pile</i> (noté FP) et <i>Face</i> puis <i>Face</i> (noté FF).
        </p>
        <p>
            Si l'on procéde à de multpiles tirages avec deux pièces, mettons 10000, on s'attend à observer PP, PF, FP et FF
            un nombre similaire de fois. Ces quatre arrangements ayant en effet chacun une chance sur quatre d'être obtenu. 
            On dit qu'ils sont équiprobables, avec la probabilité égale à `1/4`.
        </p>

        <p>
            Le graphique ci-dessous est là encore le résultat d'une simulation qui a réalisé 10000 tirages avec deux
            pièces. Il montre le nombre de fois où chacun des quatre arrangements a été observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2_tous_tirages.png" style="width: 300px;">
        </center>

        <p>
            On peut remarquer que chaque arrangement FF, FP, PF et PP a été obtenu environ 2500 fois, soit près d'un quart
            du temps, comme le laissait prévoir la probabilité `1/4`.
        </p>

        <p>
            Dans l'expérience avec deux pièces, FP et PF ont chacun une pièce sur <i>Pile</i> 
            et une pièce sur <i>Face</i>. Ainsi, on peut dire que FP et PF représentent deux réalisations possibles du 
            tirage avec une pièce sur <i>Pile</i> et une pièce sur <i>Face</i>. 
            En revanche, PP est la seule réalisation possible du tirage avec les deux pièces sur <i>Pile</i>. 
            De même, FF est la seule réalisation possible du tirage avec les deux pièces sur <i>Face</i>. 
        </p>

        <p> 
            Approfondissons notre discussion avec l'exemple d'un jeu à trois pièces.
            Tous les tirages possibles sont représentés sur l'illustration ci-dessous. 
            On peut regrouper des arrangements pour donner une description globale des tirages.
            En effet, il existe trois tirages avec une pièce sur <i>Pile</i> et deux pièces sur <i>Face</i>.
            Il existe trois tirages avec deux pièces sur <i>Pile</i> et une sur <i>Face</i>.
            En ce qui concerne les tirages avec les trois pièces identiques, ils sont uniques.
        </p>

        <center>
            <img src="images_entropie/Tirages_3_pieces.png" style="width: 300px;">
        </center>

        <br />
        
        <p>
            Les arrangements qui donnent une description globale d'un groupe de tirages sont appelés des <b>macro-états</b>.
            Les arrangements qui décrivent les tirages de façon détaillée sont appelés des <b>micro-états</b>.
            Par exemples, les tirages <i>Face Pile Pile</i> (noté FPP), <i>Pile Face Pile</i> (noté PFP) 
            et <i>Pile Pile Face</i> (noté PPF) sont les trois micro-états qui correspondent au macro-état 
            avec deux pièces sur <i>Pile</i> et une pièce sur <i>Face</i> (noté 2P1F).
        </p>

        <p>
            En reconsidérant les tirages avec trois pièces, on remarque qu'il y a trois macro-états 
            qui possèdent chacun un ou plusieurs micro-états.
        </p>

        <p>
            Revenons aux tirages avec deux pièces.
            Les micro-états PF et FP correspondent au macro-état avec une
            pièce sur <i>Pile</i> et une pièce sur <i>Face</i> (1P1F). 
            Les trois macro-états du jeu à deux pièces sont donc :
            deux fois <i>Pile</i> (2P), deux fois <i>Face</i> (2F) et une fois <i>Pile</i> et une fois <i>Face</i> (1P1F).
            Leurs probabilités respectives sont `1/4`, `1/4` et `1/2`.
        </p>

        <p>
            La simulation avec 10000 tirages de deux pièces a été réanalysée mais cette fois en s'intéressant aux
            macro-états.
            Le graphique ci-dessous résume cette analyse en montrant le nombre de fois où chacun des trois arrangements
            2F, 2P et 1P1F a été observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2.png" style="width: 300px;">
        </center>

        <p>
            Comme l'indiquaient les probabilités mentionnées ci-dessus,
            c'est bien l'arrangement avec une pièce <i>Pile</i> et une pièce <i>Face</i> qui est le plus observé
            et ce près de la moitié du temps (4979 fois pour la simulation présentée).
            Les combinaisons avec les deux pièces identiques 2P et 2F sont quant à elles observées un nombre similaire
            de fois, environ égal à 2500 chacune.
        </p>

        <p>
            Étudions maintenant le nombre de fois où les macro-états possibles ont été obtenus en
            utilisant 4, puis 16 pièces par tirage, toujours en ayant simulés 10000 tirages.
        </p>

        <p>
            Dans le cas du jeu à 4 pièces, on peut remarquer ci-dessous que c'est l'arrangement avec deux
            fois <i>Face</i> et deux fois <i>Pile</i> (2F2P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_4.png" style="width: 300px;">
        </center>

        <p>
            Dans le cas du jeu à 16 pièces ci-dessous, c'est l'arrangement avec huit fois <i>Face</i> et huit fois
            <i>Pile</i> (8F8P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_16.png" style="width: 350px;height: 175px;">
        </center>

        <p>
            Si nous réalisions des simulations avec un nombre encore plus grand de pièces, nous trouverions encore que
            l'arrangement non ordonné le plus observé serait celui pour lequel le nombre de fois <i>Pile</i> est égal au
            nombre de fois <i>Face</i> (pour un nombre de pièces pair). Par exemple, avec 100 pièces, nous trouverions
            que
            l'arrangement le plus probable serait 50 fois <i>Face</i> et 50 fois <i>Pile</i> (50F50P). Et si le nombre
            de pièces
            était impair ? Quels arrangements seraient les plus probables ?
        </p>

        <p>
            Résumons ce que les tirages de pièces nous apprennent.
        <ul>
            <li>
                Pour chaque pièce, la probabilité de tirer <i>Pile</i> est toujours d'une chance
                sur deux. Il en va de même avec la probabilité de tirer <i>Face</i>.
            </li>
            <li>
                Quel que soit le nombre de pièces, les arrangements ordonnés sont équiprobables avec la probabilité
                `1/2^n` où <i>n</i> est le nombre de pièces. Par exemple, dans un jeu à cinq pièces nous n'avons ni
                plus ni moins de chance de tirer PFPPP, PPFPP que de tirer FFFFF. Concrètement, une chance sur `2^5`,
                soit une chance sur 32.
            </li>
            <li>
                Si nous nous intéressons aux arrangements non ordonnés, le comportement de l'ensemble de pièces prises
                au hasard devient bien plus prédictible.
                En effet, l'arrangement non ordonné le plus probable aura autant de <i>Pile</i> que de <i>Face</i>.
                Par exemple, dans un jeu à 16 pièces, nos chances sont maximum en misant sur 8P8F alors qu'il est bien
                plus hasardeux de miser sur 16F ou même 14F2P.
            </li>

        </ul>

        </p>

        <p>
            Nous pouvons donc conclure que dans ce jeu de hasard, l'arrangement non ordonné avec autant de côtés
            <i>Pile</i>
            que de côtés <i>Face</i> a un
            statut bien particulier. C'est cet arrangement qui est le plus probable. Souvenez-vous par exemple qu'avec
            deux pièces l'arrangement 1F1P est le plus probable puisqu'il correspond à FP et PF, alors que 2P et 2F
            qui correspondent à PP et FF sont uniques. Nous verrons par le calcul que l'arrangement non ordonné le plus
            probable est en effet celui qui possède le plus grand nombre d'arrangements ordonnés.
        </p>

        <h2>
            Et l'entropie dans tout ça ?
        </h2>

        <p>
            Pour faire connaissance avec l'entropie, nous devons pénétrer sur son territoire, celui de la
            thermodynamique statistique pour y emprunter quelques éléments de langage. Notez bien que la thermodynamique
            statistique traite
            de systèmes physiques qui contiennent un nombre de particules de l'ordre du nombre d'Avogradro, donc bien
            loin des quelques pièces dont nous avons discuté jusqu'ici. Cependant, nous nous permettons d'ouvrir la
            frontière entre les deux mondes à des fins de compréhension.
        </p>

        <p>
            Jetons <i>n</i> pièces au hasard. Le nombre de pièces qui vont attérir sur une face donnée n'est
            pas contraint. Nous disons en thermodynamique statistique que c'est un degré de liberté. Dans le cas de nos
            jets
            de pièces, celui-ci peut varier entre 0 et <i>n</i>.
        </p>
        <p>
            Pour une valeur donnée du degré de liberté, il existe un unique arrangement non ordonné. Par exemple, nous
            jetons 16 pièces et définissons le nombre de pièces attérissant côté <i>Pile</i> comme degré de liberté.
            Celui-ci peut en
            principe prendre n'importe quelle valeur entre 0 et 16. Admettons que 5 pièces tombent sur <i>Pile</i>. La
            valeur 5 correspond à l'arrangement non ordonné 5P11F. En thermodynamique statistique, nous parlerions de
            macro-état pour 5P11F. Un macro-état correspond à une valeur du degré de liberté.
        </p>
        <p>
            Nous avons appris qu'un arrangement non ordonné donné correspond à des arrangements ordonnés, tous
            équiprobables. Pour 5P11F, un des 65536 arrangements ordonnés possibles est PPFFPPFFPFFFFFFF.
            En thermodynamique statistique, nous parlerions de micro-états pour les arrangements ordonnés.
        </p>
        <p>
            Pour résumer, une valeur du degré de liberté correspond
            à un macro-état, lequel correspond à un ou plusieurs micro-états.
        </p>

        <p>
            Nous avons découvert que l'ensemble de pièces jetées au hasard ne va pas attérir pour donner n'importe
            quelle valeur du degré de liberté. La valeur la plus probable que va prendre celui-ci est celle qui
            correspond au macro-état avec le plus grand nombre de micro-états. En effet, c'est ce macro-état qui est le
            plus probable. Si nous jetons par exemple 16 pièces, nous avons vu qu'il est fort probable que le nombre de
            pièce tombées côté <i>Pile</i> soit 8 car le macro-état 8F8P est le plus probable.
        </p>

        <p>
            Nous venons sans le savoir de donner une formulation du deuxième principe de la thermodynamique. Les
            systèmes tendent à
            s'équilibrer dans le macro-état le plus probable, celui qui a le plus grand nombre de micro-états.
        </p>
        <p>
            A l'instar de Boltzmann, désignons par <i>W</i> le nombre de micro-états d'un macro-état donné et par
            <i>S</i> l'entropie. La formule de Boltzmann `S = k_B ln W` permet de calculer l'entropie comme le
            produit du logarithme népérien de <i>W</i> (`ln W`) et de la constante de Boltzmann (`k_B`).
        </p>
        <p>
            Mettons de côté le logarithme et la constante `k_B` pour formuler les conclusions importantes auxquelles
            nous
            sommes parvenus : <b>Les systèmes tendent à s'équilibrer dans le macro-état qui rend l'entropie maximum.
                L'entropie d'un macro-état mesure la variété de réalisations de ce celui-ci en termes de
                micro-états.</b>
            Si une phrase devait être retenue à propos de l'entropie, ce serait cette dernière.
        </p>

        <h3>
            Prêts pour un test de compréhension ?
        </h3>

        <p>
            Pour renforcer notre compréhension de ce qui précède, intéressons-nous à un système de 20 jetons placés au
            hasard sur une grille de 10`times`10 cases, une case ne pouvant accueillir plus d'un jeton à la fois. Deux
            arrangements sont proposés ci-dessous.
        </p>

        <center>
            <img src="images_entropie/Pions_grille10x10_A.png" style="width: 250px">
        </center>
        </br>
        <center>
            <img src="images_entropie/Pions_grille10x10_B.png" style="width: 250px;">
        </center>

        </br>

        <p>
            Question (piège) : Laquelle des dispositions A ou B a la plus grande entropie ?
        </p>

        <p>
            Réponse : Ni l'une, ni l'autre. Les deux sont deux micro-états d'un même macro-état. Pour obtenir l'entropie
            de ce macro-état il faut calculer le nombre de combinaisons correspondant au placement de 20 jetons dans
            100 cases.
        </p>

        <p>
            Question (difficile) : Définissons le degré de liberté comme le nombre de jetons à placer sur la grille.
            Partant du macro-état ci-dessus avec 20 jetons sur 100 cases, comment générer un macro-état avec une
            entropie plus grande ?
        </p>

        <p>
            Indice : Remplir des cases avec des jetons est équivalent à arranger des jetons et des cases vides. Arranger
            des cases vides et des jetons, c'est très semblable à arranger des pièces côté <i>Pile</i> et des pièces
            côté <i>Face</i>.
        </p>

        <p>
            Réponse : Comme l'indice le suggère calculer l'entropie du macro-état correspondant à 20 pièces sur 100
            cases, c'est comme calculer l'entropie pour 20 pièces sur <i>Pile</i> et 80 pièces sur <i>Face</i>. Comme
            nous l'avons vu, le nombre de micro-états, et donc l'entropie, est maximum pour 50 pièces sur <i>Pile</i> et
            50 pièces sur <i>Face</i>. Partant de 20 jetons, augmenter ce nombre génère un
            macro-état d'entropie plus grande. Nous ne devons cependant pas dépasser 79 jetons. Souvenez-vous des
            pièces. Le nombre d'arrangements ordonnés pour 80P20F est le même que pour 20P80F. Ainsi l'entropie pour 20
            jetons est la même que pour 80. En conclusion, partant de 20 jetons, l'entropie augmentera en utilisant
            entre 21 et 79 jetons. Le maximum d'entropie étant obtenu avec 50 jetons.
        </p>

        <h3>
            Entropie et désordre
        </h3>

        <p>
            Il est fréquent de lire que l'entropie serait une mesure du désordre dans un système donné. Un
            exemple très qualitatif qui revient souvent est celui qu'une chambre d'enfant en désordre a une entropie
            plus grande que lorsqu'elle est rangée. La discussion ci-dessus devrait nous avoir armés pour comprendre ce
            qui
            est incorrect dans de telles affirmations.
        </p>

        <p>
            Reprenons les dispositions de jetons du paragraphe précédent. Nous pourrions dire que la disposition A est
            en désordre au contraire de la disposition B dont le motif est très structuré. Toutefois nous savons
            maintenant
            que ni la disposition A, ni la disposition B n'ont d'entropie. Ce sont deux arrangements possibles de 20
            jetons dans une grille de 100 cases. Qui plus est, elles sont équiprobables si elles sont générées au
            hasard.
        </p>
        <p>
            Revenons à la chambre d'enfant. Voyez-vous pourquoi elle n'a pas moins d'entropie quand elle est rangée
            que lorsqu'elle est en désordre ? La configuration "chambre rangée" est un micro-état du même macro-état que
            toutes
            celles jugées en désordre. Nous pouvons calculer l'entropie de la chambre avec un certain nombre
            d'objets et un certain nombre d'emplacements où les ranger, mais pour un nombre d'objets et d'emplacements
            donnés, qu'elle soit rangée ou en désordre ne change pas son entropie.
        </p>

        <p>
            Nous le voyons, penser à l'entropie en termes de mesure du désordre n'est pas adapté. En adoptant ce point
            de vue, nous nous forçons à réfléchir en termes de combinaison unique, alors que l'entropie mesure le nombre
            de combinaisons différentes qui correspondent à un arrangement donné.
        </p>

        <h3>Si vous aimez le calcul...</h3>

        <p>
            Si nous lançons quatre pièces au hasard, comment sait-on que 2F2P correspond au plus grand nombre
            d'arrangements
            ordonnés que l'on peut obtenir avec ces quatre pièces ? De manière plus générale, si nous jouons avec
            <i>n</i> pièces, si <i>k</i> pièces tombent côté <i>Pile</i>, combien d'arrangements ordonnés correspondent
            à
            <i>k</i>P(<i>n-k</i>)F ?
        </p>

        <p>
            En langage mathématique, nous recherchons le nombre de combinaisons de <i>k</i> éléments dans un ensemble de
            <i>n</i> éléments avec `k>=n`. Ce nombre est `C_n^k = {n!} / {k!(n-k!)}`.
        </p>

        <p>
            Pour le jeu à quatre pièces, le tableau ci-dessous donne les valeurs du nombre de combinaisons pour chaque
            arrangement non ordonné possible. Le lecteur averti remarquera que ces nombres sont ceux du triangle de
            Pascal pour <i>n</i>=4.
        </p>

        <table class="table table-striped">
            <thead>
                <tr>
                    <th>Arrangement </th>
                    <th>Nombre de combinaisons&nbsp;</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>4P</td>
                    <td>`C_4^0 = 1` </td>
                </tr>
                <tr>
                    <td>3P1F</td>
                    <td>`C_4^1 = 4`</td>
                </tr>
                <tr>
                    <td>2P2F</td>
                    <td>`C_4^2 = 6`</td>
                </tr>
                <tr>
                    <td>1P3F</td>
                    <td>`C_4^3 = 4`&nbsp;</td>
                </tr>
                <tr>
                    <td>4F</td>
                    <td>`C_4^4 = 1`</td>
                </tr>
            </tbody>
        </table>

        <p>
            Nous constatons bien que pour quatre pièces, le nombre de combinaisons maximum est obtenue avec deux pièces
            côté <i>Pile</i> et deux pièces côté <i>Face</i> (2P2F).
        </p>

        <p>
            Comment démontrer dans le cas général où <i>n</i> pièces (<i>n</i> pair) sont jetées que c'est l'arrangement
            avec `n/2` pièces sur <i>Pile</i> qui a le plus grand nombre d'arrangements ordonnés ? Il s'agit de
            rechercher pour quelle valeur de <i>k</i> `C_n^k` est maximum et de montrer que ce maximum est unique. Cela
            requiert plusieurs lignes de calcul et un peu de temps.
        </p>

        <p>Question : Une simulation place des jetons au hasard sur une grille de 16 cases. De combien augmente
            l'entropie en passant de trois à huit jetons ?</p>

        <p>Réponse : Avec trois jetons, l'entropie est proportionnelle à `ln C_16^3 = ln 560`. Avec huit jetons elle est
            proportionnelle à `ln C_16^8 = ln 12870`. En passant de trois jetons à huit jetons, l'entropie a été
            multipliée par environ 1,5.</p>

        <p>Question : Une simulation place au hasard 4 jetons sur une grille de 16 cases. Quelle est la probabilité
            d'obtenir un jeton à chaque coin ? La probabilité d'obtenir les quatre jetons au centre ?</p>

        <p>Réponse : Les deux dipositions "un jeton dans chaque coin" et "tous les jetons au centre" sont équiprobables.
            Le nombre de combinaisons qui correspondent à quatre jetons sur 16 cases est `C_16^4 = (16!)/(4!12!) =
            1820`. La probabilité d'obtenir l'une des dispositions est donc `1/C_16^4`, soit une chance sur 1820.</p>

        <p>Question : Jouons à <i>Pile ou Face</i> avec 16 pièces. Quelle est la probabilité d'obtenir 15P1F? Quelle est
            la probabilité d'obtenir 8P8F ?</p>

        <p>Réponse : Le nombre total de combinaisons possibles avec 16 pièces est `2^16 = 65536`. La probabilité
            d'obtenir 15P1F est `C_16^1 / 2^16 = 1/65536 approx 0,0015%`. La probabilité d'obtenir 8P8F est `C_16^8 /
            2^16 = 12870/65536 approx 20%`, soit plus de 10000 fois plus que la probabilité d'obtenir 15P1F.</p>

    </div>

    <div>
        <center>
            <a href="https://hits.seeyoufarm.com">
                <img
                    src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fjoulik.github.io%2Fposts%2Fentropie.html&count_bg=%234D5FEE&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false" />
            </a>
        </center>
    </div>

    <br />

    <div class="jumbotron text-center" style="margin-bottom:0">
        <p><a href="https://joulik.github.io/">joulik.github.io</a></p>
    </div>

</body>

</html>