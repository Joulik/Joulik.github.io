<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="mystyle.css">

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="true" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=AM_CHTML"> </script>

    <script src="progressbar.js"></script>

    <title>Entropie</title>
</head>

<body>

    <div id="progress-bar"></div>

    <div class="container cmath">

        <div class="alert alert-secondary">
            <h2 class="alert-heading">L'entropie est-elle une mesure du désordre ?</h2>
        </div>

        <p>
            L'entropie est parfois présentée comme une mesure du désordre. Le but est ici de se
            convaincre qu'une telle interprétation est en général inadaptée à la compréhension de la nature de l'entropie.
        </p>

        <p> Cet article a pour ambition de permettre au lecteur profane d'appréhender l'idée d'entropie. 
            Une approche mathématique est employée. Cependant, pour l'essentiel du propos, 
            peu de calculs sont en jeu. 
        </p>
        
        <p>
            D'autres articles discuteront de l'entropie en lien avec des systèmes physiques.
        </p>
             
        <p>    
            Les résultats de simulations sont utilisés à des fins d'illustration.
            Les scripts en langage Python de ces simulations sont disponibles 
            <a href="https://github.com/Joulik/Joulik.github.io/blob/master/posts/scripts_entropie.ipynb">ici</a>.
        </p>

        <p>Bonne lecture.</p>

        <h3>
            Pile ou Face ?
        </h3>

        <p>
            Utilisons pour commencer un jeu très simple, <i>Pile ou Face</i>.
        </p>

        <p>
            Lorsqu'on lance une pièce bien équilibrée, on a une chance sur deux de tirer <i>Pile</i> 
            et une chance sur deux de tirer <i>Face</i>. En d'autres termes, la probabilité d'observer <i>Pile</i> 
            et la probabilité d'observer <i>Face</i> sont égales et valent `1/2`.
        </p>

        <p>
            Que va-t-on observer si on lance une pièce un grand nombre de fois, par exemple 10000 fois, ou si on demande à 
            10000 personnes de jeter chacune une pièce ?
        </p>

        <p>
            Réaliser 10000 lancers est une tâche longue et fastidieuse qu'un ordinateur 
            peut effectuer par le biais d'un programme informatique appellé simulation. 
            Par ce moyen, on peut réaliser en une fraction de seconde ce qui prendrait des heures dans la réalité.
        </p>
        
        <p>
            Le graphique ci-dessous a été obtenu à l'aide d'une telle simulation de 10000 lancers.
            Y sont réprésentés le nombre de fois où <i>Pile</i> et <i>Face</i> ont été observés.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_1.png" style="width: 300px;">
        </center>

        <p>
            Comme on le voit sur la gauche du graphique <i>Face</i> a été observé 5010 fois dans cette simulation particulière. 
            Sur la droite, on voit que <i>Pile</i> a été observé 4990 fois.
            Le fait que <i>Pile</i> et <i>Face</i> sont observés 
            dans des proportions très proches provient du fait que chaque lancer de pièce individuel a une probabilité 
            égale d'aboutir à <i>Pile</i> ou <i>Face</i>.
        </p>

        <p>
            Renouvelons le jeu, mais cette fois-ci en lançant successivement deux pièces et en notant l'ordre des résultats.
            Quatre observations sont possibles : <i>Pile</i> puis <i>Pile</i> (noté PP), <i>Pile</i> puis <i>Face</i> (noté PF),
            <i>Face</i> puis <i>Pile</i> (noté FP) et <i>Face</i> puis <i>Face</i> (noté FF).
        </p>
        <p>
            Si l'on procéde à de multpiles tirages avec deux pièces, mettons 10000, on s'attend à observer 
            PP, PF, FP et FF un nombre similaire de fois. 
            Ces quatre arrangements ayant en effet chacun une chance sur quatre d'être obtenu. 
            On dit qu'ils sont équiprobables, avec la probabilité égale à `1/4`.
        </p>

        <p>
            Le graphique ci-dessous est là encore le résultat d'une simulation qui a réalisé 10000 tirages avec deux
            pièces. Il montre le nombre de fois où FF, FP, PF et PP ont chacun été observés.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2_tous_tirages.png" style="width: 300px;">
        </center>

        <p>
            On peut remarquer que chaque arrangement a été obtenu environ 2500 fois, soit près d'un quart
            du temps, comme le laissait prévoir la probabilité d'une chance sur quatre.
        </p>

        <p>
            Dans l'expérience avec deux pièces, FP et PF ont chacun une pièce sur <i>Pile</i> 
            et une pièce sur <i>Face</i>. 
            Ainsi, on peut dire que FP et PF représentent deux réalisations possibles du 
            tirage avec une pièce sur <i>Pile</i> et une pièce sur <i>Face</i>. 
            En revanche, PP est la seule réalisation possible du tirage avec les deux pièces sur <i>Pile</i>, 
            de même que FF est la seule réalisation possible du tirage avec les deux pièces sur <i>Face</i>. 
        </p>

        <p> 
            Approfondissons la discussion avec l'exemple d'un jeu où l'on tire trois pièces.
            Tous les tirages possibles sont représentés sur l'illustration ci-dessous. 
        </p>

        <center>
            <img src="images_entropie/Tirages_3_pieces.png" style="width: 300px;">
        </center>

        <br />
        
        <p>
            Là encore, on peut regrouper des arrangements pour donner une description globale des tirages qui peuvent être observés.
            On note alors qu'il existe trois tirages avec une pièce sur <i>Pile</i> et deux pièces sur <i>Face</i>. 
            Ils sont entourés en bleu sur la figure.
            Il existe trois tirages, entourés en vert, avec deux pièces sur <i>Pile</i> et une pièce sur <i>Face</i>.
            En ce qui concerne les tirages avec les trois pièces identiques, trois <i>Pile</i>, entourés en rouge, 
            ou trois <i>Face</i>, entourés en orange, ils sont chacun uniques.
        </p>

        <p>
            Les arrangements qui donnent une description globale d'un groupe de tirages sont appelés des <b>macro-états</b>.
            Les arrangements qui décrivent les tirages de façon détaillée sont appelés des <b>micro-états</b>.
        </p>

        <p>    
            Par exemple, les tirages FPP, PFP et PPF sont trois micro-états. Ce sont trois micro-états du macro-état 
            2P1F avec deux pièces sur <i>Pile</i> et une pièce sur <i>Face</i>.
        </p>

        <p>
            De manière générale, on dit qu'un macro-état possède un ou plusieurs micro-états.
        </p>

        <p>
            Revenons aux tirages avec deux pièces.
            Les micro-états PF et FP sont les deux réalisations du macro-état avec une
            pièce sur <i>Pile</i> et une pièce sur <i>Face</i> noté 1P1F. 
            Les trois macro-états du jeu à deux pièces sont :
            2P pour deux fois <i>Pile</i>, 2F pour deux fois <i>Face</i> et 1P1F pour une fois <i>Pile</i> et une fois <i>Face</i>.
            Leurs probabilités respectives sont `1/4`, `1/4` et `1/2`, c'est-à-dire une chance sur quatre pour 2P et 2F, 
            et une chance sur deux pour 1P1F.
        </p>

        <p>
            La simulation avec 10000 tirages de deux pièces a été réanalysée mais cette fois en s'intéressant aux
            macro-états.
            Le graphique ci-dessous résume cette analyse en montrant le nombre de fois où chacun des trois macro-états
            2F, 2P et 1P1F a été observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_2.png" style="width: 300px;">
        </center>

        <p>
            Comme l'indiquaient les probabilités mentionnées ci-dessus,
            c'est bien le macro-état avec une pièce <i>Pile</i> et une pièce <i>Face</i> qui est le plus observé
            et ce près de la moitié du temps (4979 fois pour la simulation présentée).
            Les combinaisons avec les deux pièces identiques 2P et 2F sont quant à elles observées un nombre similaire
            de fois, environ égal à 2500 chacune.
        </p>

        <p>
            Jouons maintenant avec 4, puis 16 pièces par tirage, toujours en simulant 10000 tirages.
        </p>

        <p>
            Dans le cas du jeu à 4 pièces, on peut remarquer ci-dessous que c'est l'arrangement avec deux
            fois <i>Face</i> et deux fois <i>Pile</i> (2F2P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_4.png" style="width: 300px;">
        </center>

        <p>
            Dans le cas du jeu à 16 pièces ci-dessous, c'est l'arrangement avec huit fois <i>Face</i> et huit fois
            <i>Pile</i> (8F8P) qui est le plus observé.
        </p>

        <center>
            <img src="images_entropie/LancersParTirage_16.png" style="width: 350px;height: 175px;">
        </center>

        <p>
            Si on réalisait des simulations avec un nombre encore plus grand de pièces, on trouverait encore que
            le macro-état le plus observé serait celui pour lequel le nombre de fois <i>Pile</i> est égal au
            nombre de fois <i>Face</i> (pour un nombre de pièces pair). 
            Par exemple, avec 100 pièces, on trouverait
            que l'arrangement le plus probable serait 50 fois <i>Face</i> et 50 fois <i>Pile</i> (50F50P).
        </p>

        <p>
            Résumons ce que le jeu <i>Pile ou Face</i> nous apprend.
        <ul>
            <li>
                Pour chaque pièce, la probabilité de tirer <i>Pile</i> est toujours d'une chance
                sur deux. Il en va de même avec la probabilité de tirer <i>Face</i>.
            </li>
            <li>
                Quel que soit le nombre de pièces, tous les micro-états sont équiprobables avec la probabilité
                `1/2^n` où <i>n</i> est le nombre de pièces. Par exemple, dans un jeu à cinq pièces on n'a ni
                plus ni moins de chance de tirer PFPPP, PPFPP que de tirer FFFFF. Concrètement, une chance sur `2^5` 
                (2 puissance 5), soit une chance sur 32.
            </li>
            <li>
                Si on s'intéresse aux macro-états, le comportement de l'ensemble des pièces tirées 
                au hasard devient bien plus prédictible car tous les macro-états ne sont pas équiprobables.
                En effet, le macro-état le plus probable aura autant de <i>Pile</i> que de <i>Face</i>.
                Par exemple, dans un jeu à 16 pièces, nos chances de gagner sont maximum en misant sur 8P8F alors qu'il est bien
                plus hasardeux de miser sur 16F ou même 14F2P.
            </li>

        </ul>

        </p>

        <p>
            On peut donc conclure que dans ce jeu de hasard, le macro-état avec autant de pièces tombées côté
            <i>Pile</i> que de pièces tombées côté <i>Face</i> a un statut bien particulier.
            C'est cet arrangement qui est le plus probable. Souvenez-vous par exemple qu'avec
            deux pièces l'arrangement 1F1P est le plus probable puisqu'il correspond à FP et PF, alors que 2P et 2F
            qui correspondent à PP et FF sont uniques.
        </p>

        <p>
            Avant de parler d'entropie, réfléchissons encore un peu sur le jeu de hasard <i>Pile ou Face</i>.
            Imaginons qu'on nous propose de jouer avec huit pièces. Pour gagner, on nous donne le choix de prédire
            le détail du tirage, ce qui nous rapporterait un gain maximum, ou bien de prédire combien de <i>Pile</i> et 
            combien de <i>Face</i> seront observées, ce qui nous rapporterait un peu moins. Que vaut-il mieux faire ? 
        </p>

        <p>
            Concrètement se pose le problème de prédire quel micro-état précis va être tiré si on recherche un gain maximum, 
            ou de prédire à quel macro-état le tirage appartiendra si on vise un gain moindre.
            Chaque micro-état a une probabilité de `1/2^8`. Donc si on mise par exemple sur FPFPFPFP, 
            on a une chance sur 256 de gagner. 
            Si on mise sur le macro-état le plus probable 4P4F, notre chance de succès est 70 fois plus grande avec 27,3%, 
            soit plus d'une chance sur quatre.
            Est-ce qu'une probabilité de succès d'une chance sur quatre est suffisante pour se risquer ? 
            Peut-être vaut-il simplement mieux ne pas jouer...
        </p>

        <h2>
            Et l'entropie dans tout ça ?
        </h2>

        <p>
            Quand on a bien compris ce que sont les micro-états et les macro-états, la définition de l'entropie est 
            relativement simple. 
            <b>L'entropie mesure la variété de réalisations d'un macro-état en termes de ses micro-états.</b>
            Toute la difficulté est de définir ce qu'on entend par micro-état et macro-état 
            quand on s'intéresse à autre chose que <i>Pile ou Face</i>.
        </p>

        <p>
            Reconsidérons le jeu à quatre pièces. Le macro-état 2P2F a le plus grand nombre de micro-états.
            Il a donc une entropie plus grande que tous les autres macro-états.
            On peut également noter que le macro-état le plus probable est celui qui a le plus grand nombre de 
            micro-états. C'est donc le macro-état le plus probable qui a la plus grande entropie.
        </p>
        <!-- <p>
            Pour faire connaissance avec l'entropie, nous devons pénétrer sur son territoire, celui de la
            thermodynamique statistique pour y emprunter quelques éléments de langage. Notez bien que la thermodynamique
            statistique traite
            de systèmes physiques qui contiennent un nombre de particules de l'ordre du nombre d'Avogradro, donc bien
            loin des quelques pièces dont nous avons discuté jusqu'ici. Cependant, nous nous permettons d'ouvrir la
            frontière entre les deux mondes à des fins de compréhension.
        </p> -->

        <!-- <p>
            Jetons <i>n</i> pièces au hasard. Le nombre de pièces qui vont attérir sur une face donnée n'est
            pas contraint. Nous disons en thermodynamique statistique que c'est un degré de liberté. Dans le cas de nos
            jets
            de pièces, celui-ci peut varier entre 0 et <i>n</i>.
        </p>
        <p>
            Pour une valeur donnée du degré de liberté, il existe un unique arrangement non ordonné. Par exemple, nous
            jetons 16 pièces et définissons le nombre de pièces attérissant côté <i>Pile</i> comme degré de liberté.
            Celui-ci peut en
            principe prendre n'importe quelle valeur entre 0 et 16. Admettons que 5 pièces tombent sur <i>Pile</i>. La
            valeur 5 correspond à l'arrangement non ordonné 5P11F. En thermodynamique statistique, nous parlerions de
            macro-état pour 5P11F. Un macro-état correspond à une valeur du degré de liberté.
        </p>
        <p>
            Nous avons appris qu'un arrangement non ordonné donné correspond à des arrangements ordonnés, tous
            équiprobables. Pour 5P11F, un des 65536 arrangements ordonnés possibles est PPFFPPFFPFFFFFFF.
            En thermodynamique statistique, nous parlerions de micro-états pour les arrangements ordonnés.
        </p>
        <p>
            Pour résumer, une valeur du degré de liberté correspond
            à un macro-état, lequel correspond à un ou plusieurs micro-états.
        </p>

        <p>
            Nous avons découvert que l'ensemble de pièces jetées au hasard ne va pas attérir pour donner n'importe
            quelle valeur du degré de liberté. La valeur la plus probable que va prendre celui-ci est celle qui
            correspond au macro-état avec le plus grand nombre de micro-états. En effet, c'est ce macro-état qui est le
            plus probable. Si nous jetons par exemple 16 pièces, nous avons vu qu'il est fort probable que le nombre de
            pièce tombées côté <i>Pile</i> soit 8 car le macro-état 8F8P est le plus probable.
        </p>

        <p>
            Nous venons sans le savoir de donner une formulation du deuxième principe de la thermodynamique. Les
            systèmes tendent à
            s'équilibrer dans le macro-état le plus probable, celui qui a le plus grand nombre de micro-états.
        </p> -->
        <p>
            Reste à donner une formule pour calculer l'entropie.
            Comme l'a fait l'Autrichien Ludwig Boltzmann au XIXème siècle, 
            désignons par <i>W</i> le nombre de micro-états d'un macro-état donné et par
            <i>S</i> l'entropie. La formule de Boltzmann `S = k ln W` permet de calculer l'entropie comme le
            produit du logarithme népérien de <i>W</i> (`ln W`) et de la constante de Boltzmann (`k`).
        </p>
        <!-- <p>
            Mettons de côté le logarithme et la constante `k` pour formuler les conclusions importantes auxquelles
            nous
            sommes parvenus : <b>Les systèmes tendent à s'équilibrer dans le macro-état qui rend l'entropie maximum.
                L'entropie d'un macro-état mesure la variété de réalisations de ce celui-ci en termes de
                micro-états.</b>
            Si une phrase devait être retenue à propos de l'entropie, ce serait cette dernière.
        </p> -->

        <h3>
            Prêts pour un test de compréhension ?
        </h3>

        <p>
            Pour renforcer notre compréhension de ce qui précède, intéressons-nous à un système de 20 jetons placés au
            hasard sur une grille de 10`times`10 cases, une case ne pouvant accueillir plus d'un jeton à la fois. Deux
            arrangements sont proposés ci-dessous.
        </p>

        <center>
            <img src="images_entropie/Pions_grille10x10_A.png" style="width: 250px">
        </center>
        </br>
        <center>
            <img src="images_entropie/Pions_grille10x10_B.png" style="width: 250px;">
        </center>

        </br>

        <p>
            Question (piège) : Laquelle des dispositions A ou B a la plus grande entropie ?
        </p>

        <p>
            Réponse : Ni l'une, ni l'autre. Les deux sont des micro-états d'un même macro-état. Pour obtenir l'entropie
            de ce macro-état il faut calculer le nombre de combinaisons correspondant au placement de 20 jetons dans
            100 cases.
        </p>

        <p>
            Question (difficile) : 
            Partant du macro-état avec 20 jetons sur 100 cases, comment générer un macro-état avec une
            entropie plus grande ?
        </p>

        <p>
            Indice : Remplir des cases avec des jetons est équivalent à arranger des jetons et des cases vides. Arranger
            des cases vides et des jetons, c'est très semblable à arranger des pièces côté <i>Pile</i> et des pièces
            côté <i>Face</i>.
        </p>

        <p>
            Réponse : Comme l'indice le suggère calculer l'entropie du macro-état correspondant à 20 pièces sur 100
            cases, c'est comme calculer l'entropie pour 20 pièces sur <i>Pile</i> et 80 pièces sur <i>Face</i>. Comme
            nous l'avons vu, le nombre de micro-états, et donc l'entropie, est maximum pour 50 pièces sur <i>Pile</i> et
            50 pièces sur <i>Face</i>. Partant de 20 jetons, augmenter ce nombre génère un
            macro-état d'entropie plus grande. Nous ne devons cependant pas dépasser 79 jetons. Souvenez-vous des
            pièces. Le nombre de mirco-états pour 80P20F est le même que pour 20P80F. Ainsi l'entropie pour 20
            jetons est la même que pour 80. En conclusion, partant de 20 jetons, l'entropie augmentera en utilisant
            entre 21 et 79 jetons. Le maximum d'entropie étant obtenu avec 50 jetons.
        </p>

        <h3>
            Entropie et désordre
        </h3>

        <p>
            Il est fréquent de lire que l'entropie serait une mesure du désordre dans un système donné. Un
            exemple très qualitatif qui revient souvent est celui qu'une chambre d'enfant en désordre a une entropie
            plus grande que lorsqu'elle est rangée. La discussion ci-dessus devrait nous avoir armés pour comprendre ce
            qui est incorrect dans de telles affirmations.
        </p>

        <p>
            Reprenons les dispositions de jetons du paragraphe précédent. Nous pourrions dire que la disposition A est
            en désordre au contraire de la disposition B dont le motif est très structuré. Toutefois nous savons
            maintenant
            que ni la disposition A, ni la disposition B n'ont d'entropie. Ce sont deux micro-états possibles de 20
            jetons dans une grille de 100 cases. Qui plus est, elles sont équiprobables si elles sont générées au
            hasard.
        </p>
        <p>
            Revenons à la chambre d'enfant. Voyez-vous pourquoi elle n'a pas moins d'entropie quand elle est rangée
            que lorsqu'elle est en désordre ? La configuration "chambre rangée" est un micro-état du même macro-état que
            toutes
            celles jugées en désordre. On peut calculer l'entropie de la chambre avec un certain nombre
            d'objets et un certain nombre d'emplacements où les ranger, mais pour un nombre d'objets et d'emplacements
            donnés, qu'elle soit rangée ou en désordre ne change pas son entropie.
        </p>

        <p>
            On le voit, penser à l'entropie en termes de mesure du désordre n'est pas adapté. En adoptant ce point
            de vue, on se force à réfléchir en termes de combinaison unique, de micro-état unique, alors que l'entropie 
            mesure la variété de micro-états pour un arrangement global donné.
        </p>

        <h3>Si vous aimez le calcul...</h3>

        <p>
            Lançons quatre pièces au hasard, comment sait-on que 2F2P a le plus grand nombre
            de micro-états que l'on peut obtenir avec ces quatre pièces ? De manière plus générale, si on joue avec
            <i>n</i> pièces, si <i>k</i> pièces tombent côté <i>Pile</i>, combien de micro-états correspondent
            à
            <i>k</i>P(<i>n-k</i>)F ?
        </p>

        <p>
            En langage mathématique, nous recherchons le nombre de combinaisons de <i>k</i> éléments dans un ensemble de
            <i>n</i> éléments avec `k>=n`. Ce nombre est `C_n^k = {n!} / {k!(n-k)!}`.
        </p>

        <p>
            Pour le jeu à quatre pièces, le tableau ci-dessous donne les valeurs du nombre de micro-états pour chaque
            macro-état possible. Le lecteur averti remarquera que ces nombres sont ceux du triangle de
            Pascal pour <i>n</i>=4.
        </p>

        <table class="table table-striped">
            <thead>
                <tr>
                    <th>Macro-état </th>
                    <th>Nombre de micro-états&nbsp;</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>4P</td>
                    <td>`C_4^0 = 1` </td>
                </tr>
                <tr>
                    <td>3P1F</td>
                    <td>`C_4^1 = 4`</td>
                </tr>
                <tr>
                    <td>2P2F</td>
                    <td>`C_4^2 = 6`</td>
                </tr>
                <tr>
                    <td>1P3F</td>
                    <td>`C_4^3 = 4`&nbsp;</td>
                </tr>
                <tr>
                    <td>4F</td>
                    <td>`C_4^4 = 1`</td>
                </tr>
            </tbody>
        </table>

        <p>
            On constate bien que pour quatre pièces, le nombre de micro-états maximum est obtenu avec deux pièces
            côté <i>Pile</i> et deux pièces côté <i>Face</i> (2P2F).
        </p>

        <p>
            Comment démontrer que dans le cas général où <i>n</i> pièces (<i>n</i> pair) sont jetées c'est le macro-état
            avec `n/2` pièces sur <i>Pile</i> qui a le plus grand nombre de micro-états ? Il s'agit de
            rechercher la valeur de <i>k</i> pour laquelle `C_n^k` est maximum et de montrer que ce maximum est unique. Cela
            requiert plusieurs lignes de calcul et un peu de temps. A vos crayons !
        </p>

        <p>Question : Une simulation place des jetons au hasard sur une grille de 16 cases. De combien augmente
            l'entropie en passant de trois à huit jetons ?</p>

        <p>Réponse : Avec trois jetons, l'entropie est proportionnelle à `ln C_16^3 = ln 560`. Avec huit jetons elle est
            proportionnelle à `ln C_16^8 = ln 12870`. En passant de trois jetons à huit jetons, l'entropie a été
            multipliée par environ 1,5.</p>

        <p>Question : Une simulation place au hasard 4 jetons sur une grille de 16 cases. Quelle est la probabilité
            d'obtenir un jeton à chaque coin ? La probabilité d'obtenir les quatre jetons au centre ?</p>

        <p>Réponse : Les deux dipositions "un jeton dans chaque coin" et "tous les jetons au centre" sont équiprobables.
            Le nombre de combinaisons qui correspondent à quatre jetons sur 16 cases est `C_16^4 = (16!)/(4!12!) =
            1820`. La probabilité d'obtenir l'une des dispositions est donc `1/C_16^4`, soit une chance sur 1820.</p>

        <p>Question : Jouons à <i>Pile ou Face</i> avec 16 pièces. Quelle est la probabilité d'obtenir 15P1F? Quelle est
            la probabilité d'obtenir 8P8F ?</p>

        <p>Réponse : Le nombre total de combinaisons possibles avec 16 pièces est `2^16 = 65536`. La probabilité
            d'obtenir 15P1F est `C_16^1 / 2^16 = 16/65536 approx 0,024%`. La probabilité d'obtenir 8P8F est `C_16^8 /
            2^16 = 12870/65536 approx 20%`, soit environ 10000 fois plus que la probabilité d'obtenir 15P1F.</p>
    </div>

    <div>
        <center>
            <a href="https://hits.seeyoufarm.com">
                <img
                    src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fjoulik.github.io%2Fposts%2Fentropie.html&count_bg=%234D5FEE&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false" />
            </a>
        </center>
    </div>

    <br />

    <div class="jumbotron text-center" style="margin-bottom:0">
        <p><a href="https://joulik.github.io/">joulik.github.io</a></p>
    </div>

</body>

</html>